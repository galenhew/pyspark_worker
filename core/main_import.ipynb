{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main_import.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNjnjZP9QTdq3/FFDWDTIgi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"OCTbDqh6AUDL"},"outputs":[],"source":["\n","from datetime import date \n","import datetime as dt \n","from dateutil.relativedelta import relativedelta\n","\n","#from..config import *\n","\n","# KEY1 config[\"spark\" ][\"hadoop\"][\"key1\"] \n","# KEY2= config[\"spark\"][\"hadoop\"][\"key2\"]\n","\n","Class Sparky:\n","  \"\"\"\n","  class to initialize spark and methods.\n","\n","  Parameters\n","  --------\n","  env_keys:\n","    expects jupyter os.environ dict\n","  \"\"\"\n","  def init__(self)-> SparkSession:\n","    self.spark self.spark_session()\n","\n","  def spark_session(self): \n","    hadoop_endpoint= 'https://s3.sgp.com'\n","    presto_endpoint= 'jdbc:presto://sgp.com:8888/hive/information_schema'\n","    s3_path= 's3a://user/{tdy}'\n","\n","    session= (SparkSession\n","            .builder\n","            .config(\"spark.hadoop.ipc.client.fallback-to-simple-auth-allowed\", \"true\")\n","            .config(\"spark.network. timeout\", \"180\") .config(\"spark.dynamicAllocation.executorIdleTimeout\", \"128\")\n","            .config(\"spark.kryoserializer.buffer.max\", \"1024m\")\n","            .config(\"spark.driver.extrajavaoptions\",\"Dcom.amazonaws.sdk.disableCertChecking-true\") \n","            .config(\"spark.executor. extrajavaoptions\", Dcom.amazonaws.sdk.disableCertChecking-true\")\n","            .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFilesystem\")\n","            .config(\"spark.hadoop.fs.s3a.access.key\", os.environ[\"hadoop1\"])\n","            .config(\"spark.hadoop.fs.s3a.secret.key\", os.environ[\"hadoop2\"]).\n","            .config(\"spark.hadoop.fs.s3a.endpoint\", hadoop_endpoint) .appName (\"PrestoConnect\")\n","            .config(\"spark.executor.cores\", 1) .config(\"spark.dynamicAllocation.minExecutors\", e)\n","            .config(\"spark.dynamicAllocation.maxExecutors\", 12)\n","            .config(\"spark.executor.memory\", \"2g') .config(\"spark.yarn.executor.memoryOverhead\", \"2g')\n","            .config(\"spark.yarn.queue\",\"root.ADA_ANALYTICS_RMG\")\n","            .config(\"spark.driver.memory\", \"2g\"). .config(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n","            .config(\"spark.files\",\"/data/custom-jars/presto/presto.jks,/data/custom-jars/presto/truststore.jks,/etc/krbs.conf\") \n","            .config(\"spark.jars\", \"/data/custom-jars/presto/presto-jdbc-312-e.7.jar\") \n","            .getorCreate())\n","      return session\n","\n","  def read_table_presto(self, table_name): \n","      jdbcDF = self.spark.read \\\n","              .format(\"jdbc\") \\\n","              .option(\"driver\", \"io.prestosql.jdbc.PrestoDriver\") \\\n","              .option(\"url\", presto_endpoint) \\\n","              .option(\"SSL\",\"true\") \\\n","              .option (\"SSLTrustStore Path\", \"/data/custom-jars/presto/truststore.jks\") \\\n","              .option(\"SSLTrustStorePassword\", \"changeit\") \\\n","              .option(\"SSLKeyStorePath\",\"/data/custom-jars/presto/presto.jks\") \\\n","              .option(\"SSLKeyStore Password\", \"changeit\") \\\n","              .option(\"dbtable\", table_name) \\\n","              .option(\"username\", os.environ[\"username\"]) \\\n","              .option(\"password\", os.environ[\"password\"]) \\\n","              .load() \n","      return jdbcDF\n","\n","  def push_to_s3(self, query, table): \n","    tdy= date.today().strftime('%Y%m%d')\n","    tmp_path = s3_path\n","    query.repartition(1).write\\\n","      .option(\"mapreduce.fileouputcommitter.algorithm.version\", \"2\")\\\n","      .option(\"header\", \"true\")\\ .format('com.databricks.spark.csv')\\\n","      .save(fr\"{tmp_path}/{table}\", mode = \"overwrite\")\n","\n","    Path = sc. _gateway.jvm.org.apache.hadoop.fs.Path \n","    fs = Path(tmp_path).getFileSystem (self.spark._jsc.hadoopConfiguration()) \n","    csv_part_file = fs.globStatus (Path (f'{tmp_path}/{table}' + \"/part*\"))[0].getPath() \n","    fs.rename(csv_part_file, Path (f' {tmp_path}/{table}.csv')) \n","    fs.delete (Path (f'{tmp_path}/{table}'), True)\n","\n"]}]}