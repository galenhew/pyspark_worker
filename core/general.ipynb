{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"general.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNdHxwfm2KSIxgtTk6DGCxv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"BcCfs2HZkkQ2"},"outputs":[],"source":["import pandas as pd\n","\n","from pyspark.sql.types import * \n","# from pyspark.sql.types import StringType, IntegerType, ArrayType, DataType \n","from pyspark.sql import Column, functions as F, DataFrame, Window\n","\n","from functools import wraps, reduce\n","from time import time\n","from typing import Callable\n","\n","from itertools import chain\n","from datetime import date\n","import datetime as dt\n","from dateutil.relativedelta import relativedelta\n","\n","\n","class ExecTiming:\n","\n","  def _init__(self):\n","    return None\n","\n","  def timing (f):\n","    \"\"\"\n","    function to see execution time of functions\n","    \"\"\"\n","    @wraps(f)\n","    def wrap(*args, **kw):\n","      ts= time()\n","      result = f(*args, **kw)\n","      te = time()\n","      print (f'Function {f._name__} took {te-ts:2.4f} seconds') \n","      return result\n","    \n","    return wrap\n","\n","\n","class PyDatetime:\n","\n","  \"\"\" \n","  class for Datetime manipulation\n","\n","  Parameters\n","  ---------\n","  input\n","    date in str format %Y-%m-%d\n","\n","  ymd\n","    str (yyyy-mm-dd) / %Y-%m-%d\n","\n","  timestamp_col \n","    TimestampType attribute is dataframe column created from string date\n","  \"\"\"\n","\n","  def _init__(self, ymd=None): \n","    self.date_input= ymd\n","    self.timestamp_col= self.new_timestamp_col(self.date_input) \n","    self.lday= self.new_lday(self.date_input)\n","\n","  @property\n","  def date_input (self): \n","      return self._date_input\n","\n","  @date_input.setter\n","  def date_input (self, value):\n","    # validation\n","    try:\n","      if value is None: \n","        self._date_input \n","        date.today().strftime('%Y-%m-%d')\n","\n","      elif isinstance (dt.datetime.strptime(value, '%Y-%m-%d').date(), date): \n","        self._date_input = value \n","    except ValueError:\n","      raise ValueError('Expected a string with format %Y-%m -%d')\n","\n","\n","  def new_timestamp_col(self, value :str) -> TimestampType:\n","\n","    \"\"\"\n","    return:\n","    pyspark TimestampType column '%Y-%m-%d'\n","    \"\"\"\n","    self._timestamp_col = F.to_date(F.lit(value)).cast(TimestampType())\n","    return self._timestamp_col\n","\n","  def new_lday(self, value: str, num_months: int=12)-> list:\n","    \"\"\"\n","    method to find last day of each month\n","\n","    return: dictionary with list of dates for first day and last day\n","    \"\"\"\n","\n","    clean_date= dt.datetime.strptime (value, '%Y-%m-%d').date()\n","\n","    first_day_list = [] \n","    last_day_list = []\n","\n","    first_day= clean_date.replace(day=1) \n","    last_day= first_day - dt.timedelta(days=1)\n","\n","    for i in range(num_months):\n","      last_month= first_day- relativedelta(months=i) \n","      first_day_list.append(last_month)\n","\n","      last_day = first_day_list[i] - dt.timedelta(days=1)\n","      last_day_list.append(last_day)\n","\n","    result= {'first': first_day_list, 'last': last_day_list} \n","    return result\n","\n","\n","class CustomUdf:\n","  \"\"\"\n","  class for Datetime manipulation\n","\n","  Parameters\n","  -----------\n","    timestamp: str\n","      Create TimestampType attribute from string date Return\n","    return\n","      udf function or python function, depending on DataFrame column\n","  \"\"\"\n","\n","  def _init_(self, returnType: DataType=StringType()):\n","    self.spark_udf_type = returnType\n","\n","  def _call_(self, func: Callable): \n","    def wrapped_func (*args, **kwargs):\n","      if any([isinstance(arg, Column) for arg in args]) or \\\n","        any ( [isinstance (vv, Column) for vv in kwargs.values()]): \n","        return F.udf(func, self.spark_udf_type) (*args, **kwargs)\n","      else: \n","        return func(*args, **kwargs)\n","    return wrapped_func\n","\n","## pivottable class\n","\n","class PivotTable:\n","  \"\"\"\n","  notes: name columns without special chars (i.e..') else cannot resolve column name\n","\n","  col*\n","    col[0] 1st level \n","    col [1] 2nd level\n","\"\"\"\n","\n","  def _init_(self, df: DataFrame, row: list= None, value=None , col: list =None):\n","    self._df = df\n","    self._row= row\n","    self._value= value\n","    self._col= col\n","\n","  def add_totals(self, df) -> DataFrame:\n","    cols= df.columns\n","    total_row= df.agg( *((F.sum(col)) .alias (col) for col in cols) ) \n","    total_row= total_row.withColumn (df. columns [0], F.lit('Total'))\n","    result= df.union (total_row)\n","\n","    return result\n","\n","\n","  def pivot_1(self) -> DataFrame: \n","    df= self._df \\\n","        .select(*self._row, self._value)\\\n","        .groupBy (self._row[0])\\\n","        .agg (F.sum (self._value).alias('$'), F.count(F.lit (1)). alias ('#'))\\\n","        .sort (F.desc('$'))\n","\n","    df= self.add_totals (df)\n","\n","    return df\n","\n","\n","  def pivot_2(self) -> DataFrame:\n","\n","    if self._col is not None: \n","      df= self._df \\\n","        .groupBy (self._row[0], self._col[1]).pivot (self._co1[0])\\\n","        .agg( F.sum(self._value).alias ('$'), F.count(F.lit (1)).alias('#'))\n","\n","      agg_cols = [x for x in df.columns if x not in [self._row[0], *self.__col]]\n","\n","      df= df\\\n","        .groupBy(self._row[0],)\\\n","        .pivot (self._col[1])\\\n","        .agg( *[F.sum (F.col (x)).alias(x) for x in agg_cols])\n","\n","      df= self.add_totals (df)\n","      \n","      return df\n","    \n","    else:\n","      print('column list required')\n","\n","\n","  def pivot_all(self):\n","    return self.pivot_1(), self.pivot_2()\n","\n","class Utilities:\n","  \"\"\"\n","  Method: map_create_col\n","\n","  Purpose: adds new column that contains the new mapped values, to dataframe\n","\n","  Parameters\n","\n","  map_dict: dict\n","    dictionary with map of old: new \n","  \n","  new_col_name: str \n","    new column name in string\n","\n","  \"\"\"\n","\n","  def __init__(self):\n","    return None\n","\n","  def map_create_col(self, df: DataFrame, map_dict: dict, new_col_name: str, old_col_name: str)-> DataFrame:\n","    mapping_expr= F.create_map([F.lit(x) for x in chain(*map_dict.items())])\n","    df = df.withColumn (new_col_name, mapping_expr[F.col (old_col_name)]) \n","    return df\n","\n","  def neg_to_zero(self, df: DataFrame, col_name: str)-> DataFrame:\n","    df1= df.withColumn (col_name, \n","                      F.when (F.col(col_name) <0, 0)\\\n","                      .otherwise (F.col(col_name))\n","          )\n","    return df1\n","\n","\n","  def create_index(df: DataFrame, index_name: str)-> DataFrame: \n","    df = df.withColumn(index_name, \n","                    F.row_number().over(Window.orderBy(F.monotonically_increasing_id()))-1 )\n","    return df\n","\n","\n","  def string_to_list(long_string: str)-> list:\n","    \"\"\"\n","    purpose:  \n","      copy paste select columns from SAS\n","    \"\"\"\n","    result= [i for i in long_string.split()] \n","    return result\n","\n","\n","class Joins:\n","  \"\"\"\n","  note:\n","  https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.union.html\n","  pyspark union and unionall behave the same, does not eliminate duplicate. need to remove.\n","  \"\"\"\n","\n","  @staticmethod\n","  def union(*df) -> DataFrame:\n","    \"\"\"\n","    df:\n","\n","    list of dataframes. note that order of columns shld be same. else use unionByName\n","    \"\"\"\n","\n","    result= reduce (DataFrame.union, df)\n","    return result\n","\n","  @staticmethod\n","  def union_all(*df) -> DataFrame:\n","    \"\"\"\n","    df: list of dataframes. note that order of columns shld be same. else use unionByName\n","    \"\"\"\n","    result= reduce (DataFrame.unionAll, df)  \n","    result= result.distinct()\n","    return result\n","\n","\n","\n","class EDA:\n","  \"\"\"\n","  class to make it ez to check data\n","  \"\"\"\n","  def _init_(self, df): \n","    self. df = df\n","\n","  def summary_stat (self):\n","    return self._df.summary(\"count\", \"min\", \"25%\", \"75%\", \"max\")\n","\n","  def col_zoom(self, col_name):\n","    result= self._df\\\n","            .select(col_name)\\\n","            .where( F.col(col_name).isNotNull())\\\n","            .toPandas ()\n","    return result\n","\n","\n","\n","  def col_zoom_nozero (self, col_name):\n","          \n","    result= self._df\\\n","            .select(col_name)\\\n","            .where( F.col(col_name).isNotNull())\\\n","            .where( F.col(col_name) > 0)\\\n","            .toPandas()\n","    \n","    return result\n","\n","\n","class SasPandas:\n","  \"\"\"\n","  purpose:\n","\n","  convert sas or panda datasets to spark or other formats.\n","    \"\"\"\n","  def _init_(self, spark_instance):\n","    \"\"\"\n","    spark_instance:\n","      need spark instance for spark methods\n","    \"\"\"\n","    self._spark = spark_instance\n","\n","\n","  def excel_to_pandas (self, file_path: str):\n","    \"\"\"\n","    file_path:\n","      '/MAS606 - with formulas v2.xls'\n","  \n","    errors:\n","      if xlrd engine not installed, ensure installed via pip3 \n","      if openpyxl engine not installed, ensure installed via pip3\n","    \"\"\"\n","    if file_path.endswith('.xls'):\n","      print('xls file recognized') \n","      engine_option= 'xlrd' \n","    elif file_path.endswith('.xlsx'): \n","      print('xlsx file recognized') \n","      engine_option = 'openpyxl'\n","\n","    result= pd.read_excel(file_path, engine= engine_option)\n","    return result\n","\n","\n","  def sas_to_pandas (self, file_path: str): \n","    \"\"\"\n","    Given pandas dataframe, it will return a spark's dataframe. \n","    \"\"\"\n","    result= pd.read_sas(file_path, encoding='ISO-8859-1')\n","    return result \n","\n","    \n","  def pandas_to_spark(self, df) -> DataFrame:\n","    \"\"\"\n","    df:\n","      pandas dataframe\n","    \"\"\"\n","    \n","    columns = list(df.columns)\n","    types= list(df.dtypes)\n","    struct_list= [] \n","    for column, typo in zip(columns, types): \n","      struct_list.append(self.define_structure (column, typo)) \n","    \n","    p_schema= StructType (struct_list)\n","    return self._spark.spark.createDataFrame(df, p_schema)\n","\n","  def sasxls_to_spark(self, file_path) -> DataFrame:\n","    \"\"\"\n","    file_path:\n","      either excel or sas extension \n","      eg. '/dataset.sas7bdat' or 'excel.xls'\n","    \"\"\"\n","    if file_path.endswith(('.xls', '.xlsx')): \n","      df= self.excel_to_pandas (file_path) \n","      result = self.pandas_to_spark(df) \n","      return result\n","      \n","    elif file_path.endswith('.sas7bdat'): \n","      df= self.sas_to_pandas (file_path) \n","      result = self.pandas_to_spark(df) \n","      print('sas file recognized')\n","\n","\n","  def equivalent_type(self, f):\n","    if f == 'datetime64 [ns]': return TimestampType() \n","    elif f == 'int64': return LongType() \n","    elif f == 'int 32': return IntegerType() \n","    elif f == 'float64': return FloatType()\n","    else: return StringType()\n","\n","\n","  def define_structure(self, string, format_type): \n","    try: typo= self.equivalent_type (format_type) \n","    except: typo= StringType() \n","    return StructField(string, typo)\n","\n","\n","  def get_parquet (self, yyyymmdd: str, table: str)-> DataFrame:\n","    \"\"\"\n","    purpose:\n","      get parquet quick.\n","    issue:\n","      insufficient parquet files now. use 1 month first.\n","    \"\"\"\n","    table_list= ['AMBS', 'AMNA', 'AMPS']\n","    \n","    if table in table_list:\n","      path= \"alluxio :///rmgcbgcredit/credit/vplus/\" \n","      ext= \".parquet\"\n","      output= self._spark.spark.read.parquet (path + table + '20220131' + ext) \n","      return output\n","    else:\n","      print('Please input table name ass one of deze', table_list) \n","      return None\n","\n","\n","\n","class SparkDatatype:\n","  \"\"\"\n","  purpose:\n","    convert spark datatype, when you already have spark dataframe.\n","    i.e.\n","    int_cast=[]\n","\n","    decimal_cast-[\n","    \"total_bal', AMPS USER_6_BNP',\n","    'credit_limit', AMPS USER_5_BNP*\n","\n","    char_cast-[\n","    'block_code_1', 'block_code_2\",]\n","\n","    date_cast=[AMBS_DATE_OPENED',AMBS_DATE_CGOFF',]\n","    a= SparkDatatype().col_cast(final_all, date_cast= date_cast, \n","                                    int_cast= int_cast, \n","                                    decimal_cast= decimal_cast, \n","                                    char_cast= char_cast)\n","\n","    \n","  \"\"\"\n","\n","\n","  # def col_check (self, all_cols, *data_types): \n","  #   sum=[]\n","  #   for i in data_types: \n","  #       sum sum + i\n","  #   diff= set(all_cols)- set(sum)\n","  #   print (diff)\n","\n","  def caster(self, col_to_cast: list, df: DataFrame, cast_type: str) -> DataFrame: \n","    for i in col_to_cast:\n","      if cast_type == 'date':\n","        df= df.withColumn(i,\n","                    F.to_date (F.unix_timestamp (F.col (i), 'yyyy-MM-dd').cast(\"timestamp\"))) \n","      \n","      else: \n","        df-df.withcolumn(i, F.col (i). cast(cast_type))\n","\n","    output= df \n","    return output\n","\n","\n","  def col_cast(self, df: DataFrame, **cast_lists) -> DataFrame:\n","    df= self.caster(cast_lists ['date_cast'], df, 'date')\n","    df= self.caster(cast_lists['int_cast'], df, 'int')\n","    df= self.caster(cast_lists['decimal_cast'], df, 'decimal(15,2)') \n","    df= self.caster(cast_lists ['char_cast'], df, 'string') \n","    return df\n","\n","\n","class String0p:\n","  \"\"\"\n","  purpose:\n","\n","    functions for common pyspark string manipulation\n","\n","  1.e.\n","    c= fi_keys.withColumn('a', F.lit('abiaw oifwefoiw'))\\\n","            .withColumn('b', F.col('Financial Institutions'))\\\n","            .withColumn('c', F.concat('a', 'b', 'Financial Institutions'))\n","    concat_list= sorted(c.columns)\n","    d = G.StringOp.concat_no_space(c, 'e', concat_list)\n","  \"\"\"\n","\n","  @staticmethod\n","  def concat_no_space(df: DataFrame, col_name: str, concat_list: list) -> DataFrame:\n","\n","    df= df\\\n","      .withColumn (col_name,\n","        F.regexp_replace(F.concat(*[F.col (i) for i in concat_list]), \" \", \"\"))\n","    return df\n","\n","\n","  @staticmethod\n","  def acc_concat (non_col: list, full_list: list, delimiter: str)-> DataFrame:\n","    \"\"\"\n","    purpose:\n","    -----------\n","    ez way to concat string literals and columns, to make new column\n","\n","    parameters:\n","    ----------\n","    full list\n","      input full list in order\n","\n","    non_col:\n","      string literals to join\n","\n","    assume:\n","      full list> non-col list\n","    \"\"\"\n","\n","    col_list= list(set (full_list)- set(non_col))\n","    concat_list= []\n","\n","    for i in full_list: \n","      if i in non_col:\n","        concat_list.append(F.lit(i)) \n","      else:\n","        concat_list.append(F.col(1))\n","\n","    result= F.regexp_replace(F.concat(*concat_list), delimiter, \"\")\n","    return result"]}]}